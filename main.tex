% Template:     Tesis LaTeX
% Documento:    Archivo principal
% Versión:      3.3.1 (21/08/2023)
% Codificación: UTF-8
%
% Autor: Pablo Pizarro R.
%        pablo@ppizarror.com
%
% Manual template: [https://latex.ppizarror.com/tesis]
% Licencia MIT:    [https://opensource.org/licenses/MIT]

% CREACIÓN DEL DOCUMENTO
\documentclass[
	spanish, % Idioma: spanish, english, etc.	
	letterpaper, oneside
]{book}

% INFORMACIÓN DEL DOCUMENTO
\def\documenttitle {EXTENDING REINFORCEMENT LEARNING TECHNIQUES FOR DIFFUSION MODELS}
\def\documentsubtitle {}
\def\degreetitle {
Tesis para optar al grado de magíster en ciencia de datos
%	\bigbreak\vspace{0.3cm}
%	Memoria para optar al título de ingeniero
}

\def\universityname {Universidad de Chile}
\def\universityfaculty {Facultad de Ciencias Físicas y Matemáticas}
\def\universitydepartment {Departamento de Postgrado y Postítulo}
\def\universitydepartmentimage {departamentos/uchile2}
\def\universitydepartmentimagecfg {height=3cm}
\def\universitylocation {Santiago de Chile}

% INTEGRANTES, PROFESORES Y FECHAS
\def\documentauthor {Cristóbal Patricio Alcázar Carrasco}
\def\documentdate {\the\year}

\def\portrait {
	\begin{center}
	\vspace{1.5cm} ~ \\
	\MakeUppercase{\textbf{\documenttitle}} ~ \\
	\vspace{1.5cm}
	\MakeUppercase{\degreetitle} ~ \\
	\vfill
	\begin{tabular}{c}
		\MakeUppercase{\textbf{\documentauthor}} \\ \\
		\vspace{1.0cm} \\
		PROFESOR GUÍA: \\
		FELIPE TOBAR \\
		\vspace{0.5cm} \\
		MIEMBROS DE LA COMISIÓN: \\
		PROFESOR 2 \\
		PROFESOR 3 \\
		\vspace{0.5cm} \\
		Este trabajo ha sido parcialmente financiado por: \\
		NOMBRE INSTITUCIÓN \\
		\vspace{0.5cm} \\
		\MakeUppercase{\universitylocation} \\
		\MakeUppercase{\documentdate}
	\end{tabular}
	\end{center}
}

% abstract en ingles y español
\def\abstracttable {
	\begin{tabular}{l}
		THESIS SUMMARY TO QUALIFY FOR \\
		THE DEGREE OF MASTER OF SCIENCE \\
		IN DATA SCIENCE \\
		BY: \MakeUppercase{\documentauthor} \\
		DATE: \MakeUppercase{\documentdate} \\
		ADVISOR: FELIPE TOBAR
		%PROF. GUÍA: Felipe Tobar
	\end{tabular}
}

\def\abstracttableesp {
	\begin{tabular}{l}
		RESUMEN DE LA TESIS PARA OPTAR \\
		AL TÍTULO DE MAGÍSTER EN CIENCIAS \\
		DE DATOS \\
		POR: \MakeUppercase{\documentauthor} \\
		FECHA: \MakeUppercase{\documentdate} \\
		PROF. GUÍA: FELIPE TOBAR
	\end{tabular}
}

\def\documenttitleesp {
	EXTENSIÓN DE TÉCNICAS DE APRENDIZAJE POR REFUERZO PARA MODELOS DE DIFUSIÓN
}

% IMPORTACIÓN DEL TEMPLATE
\input{template}

% INICIO DE LAS PÁGINAS
\begin{document}

% PORTADA
\templatePortrait

% CONFIGURACIÓN DE PÁGINA Y ENCABEZADOS
\templatePagecfg

% RESUMEN O ABSTRACT
\begin{abstractd}
	Reinforcement Learning (RL) has become a pivotal tool for aligning complex generative models, addressing the limitations of traditional supervised learning methods. Its capability to optimize arbitrary rewards, including non-differentiable scalar functions or human feedback, is particularly useful for large-scale models such as Large Language Models (LLMs) and diffusion models. This thesis investigates the application of RL techniques to pretrained diffusion models, employing policy gradient methods to adapt these models for new tasks. It explores how diffusion models can be viewed as agents generating samples to maximize specific attributes—--such as aesthetic quality or compressibility--—and conducts an empirical analysis of reward signals over sample trajectories. The work includes the implementation of state-of-the-art policy optimization algorithms (DDPO) and integrates human feedback to provide practical tools and insights. This research offers a pathway for understanding the use of RL in finetuning pretrained diffusion models and provides insights for potential future adaptations.
\end{abstractd}

\begin{abstractdesp}
	El Aprendizaje por Refuerzo (RL) se ha convertido en una herramienta crucial para alinear modelos generativos complejos, superando las limitaciones de los métodos de aprendizaje supervisado tradicionales. Su capacidad para optimizar recompensas arbitrarias, incluyendo funciones escalares no diferenciables o retroalimentación humana, es especialmente útil para modelos a gran escala como los Modelos de Lenguaje Grandes (LLMs) y los modelos de difusión. Esta tesis investiga la aplicación de técnicas de RL a modelos de difusión preentrenados, utilizando métodos de gradiente de políticas para adaptar estos modelos a nuevas tareas. Explora cómo los modelos de difusión pueden considerarse agentes que generan muestras para maximizar atributos específicos, como la calidad estética o la compresibilidad, y realiza un análisis empírico de las señales de recompensa a lo largo de las trayectorias de muestra. El trabajo incluye la implementación de algoritmos de optimización de políticas de vanguardia (DDPO) e integra la retroalimentación humana para proporcionar herramientas y perspectivas prácticas. Esta investigación ofrece una ruta para comprender el uso de RL en el ajuste de modelos de difusión preentrenados y proporciona ideas para posibles adaptaciones futuras.
\end{abstractdesp}

% DEDICATORIA
\begin{dedicatory}
	A mi familia \\
	% Una frase de dedicatoria, \\
	% pueden ser dos líneas. \\
	~ \\
	% \textbf{Saludos}
\end{dedicatory}

% AGRADECIMIENTOS
%\begin{acknowledgments}
	% \ca{Debo agregar algo del fondo de investigación de la beca?}
%	\lipsum[1]
%\end{acknowledgments}

% TABLA DE CONTENIDOS - ÍNDICE
\templateIndex

% CONFIGURACIONES FINALES
\templateFinalcfg

% ======================= INICIO DEL DOCUMENTO =======================
\input{ch1-intro}

\input{ch2-diffusion-models}

\input{ch3-rl}

\input{ch4-extending-rl-in-dm}

\input{ch5-conclusions}

\typeout{}\bibliography{library}

\input{appendix}

% FIN DEL DOCUMENTO
\end{document}