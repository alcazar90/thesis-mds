\chapter{Introduction}

\section{Overview}

\begin{chapquote}{Lewis Carroll, \textit{Alice in Wonderland}}
``Begin at the beginning,'' the King said, gravely, ``and go on till you come to an end; then stop.''
\end{chapquote}

\insertimage[\label{fig:anatomy-rl-algo}]{ch3-rl/anatomy-rl-algorithms.png}{scale=0.85}{Adapted from the Sergey Levine Course on Deep Reinforcement Learning}

\ca{Cambiar toda la sección de overview...}.

The \textbf{good}: Formular la cadena de difusión como un \textit{Markov Decision Process} (MDP) permite utilizar toda la bateria de Reinforcement Learning; Esto en terminos de modelos de Difusión pre-entrenados es la posibilidad de adoptar sus capacidades a una nueva tarea, concepto, o función como se demuestre en \cite{black2023training}. Incluso, es posible realizar alignment de \textit{text-to-image} models usando \textit{feedback} humano \cite{ouyang2022training}. \\

The \textbf{bad}: La Figura~\ref{fig:anatomy-rl-algo} ilustra la anatomía de cualquier algoritmo de reinforcement learning (e.g. REINFORCE, DQN) para aprender un \textit{policy} a traves de interactuar con su ambiente (real o simulado). \\

%The \textbf{thesis:} ¿Qué tecnicas se pueden utilizar para disminuir el costo de simular la interacción? ¿Es posible utilizar alguna técnica de neural compression (e.g. AE, VAE, VQ-VAE/VQ-GAN) como se usa en LDM \cite{rombach2022highresolution} o \cite{pernias2023wuerstchen} para entrenar el modelo, pero a nivel de fine-tuning. Un reward asigna un score a cada trayectoria (simulación), es consumidor de trayectorias, se puede utiliar un meta reward para ir destilando trayectorias más cortas a medida que el algoritmo aumenta? \\

\ca{Probablemente acá sera interesante poner ese diagrama de Venn con la interessción de modelos de difusión y RL...(?)}

\section{Related Work}

\ca{En esta sección incorporar el related work de reinforcement learning, reinforcement learning from human feedback, esta se puede completar antes 
que la sección de arriba. \textbf{TODO:} basarse en exposición y blogpost de \href{https://yang-song.net/blog/2021/score/}{Generative Modeling by Estimating Gradients of the Data Distribution} (Yang Song).}

Diffusion \citep{sohldickstein2015deep} \citep{ho2020denoising} and score-based models \citep{song2020generative} are a kind of generative models, in which they learn from the dataset $\mathcal{D}$ a distribution $p(\mathbf{x})$ and achieve the two basic primitive operations expected for this type of models, evaluation and sampling, with the importance that scale well and handle highly complex data such as images or audio. To accomplish this task, diffusion models start with a simple prior distribution (e.g. isotropic-gaussian) and, iteratively, through a denoising process, it mutates into the target distribution. Recent years have shown an essential advance in extending and adapting the work on diffusion models, likewise making the sampling process faster in the work of \cite{song2020denoising}, \cite{nichol2021improved}, and \cite{Salimans2022ProgressiveDF}. Incorporate additional information $\mathbf{y}$ (e.g. text prompt, an image seed) to learn a conditional model $p(\mathbf{x} | \mathbf{y})$; this endeavour will be explored indirectly by guiding the sampling with an external classifier in \cite{Dhariwal2021DiffusionMB}, or achieving the same but directly in the model training phase using classifier-free guidance purpose in \cite{Ho2022ClassifierFreeDG} and effectively implemented in work \cite{nichol2022glide}. These techniques had many connections with the concept of temperature commonly used in Large Language Models (LLM) and proved helpful to guide the sampling in the gradient direction toward the space between $\mathbf{x}$ and $\mathbf{y}$ signals. Moreover, progress toward improving the likelihood estimation and exhibited impressive results on tasks such as text conditional image generation with visual language models \cite{ramesh2022hierarchical} and \cite{saharia2022photorealistic}, super-resolution \cite{ho2021cascaded}, in-painting, style transfer, and combining different data modalities. \\

A vital research line is to use pre-trained diffusion models and adapt to novel tasks or concepts. Given the high cost of data recollection, the quantity and quality of these, and the computational resources it takes to train these "lossless" generative models from scratch. The idea is to push forward in the line that follows the LLM and their adaptation to downstream tasks \citep{Radford2019LanguageMA}. Specifically, the work of \cite{ruiz2023dreambooth} using a small number of images 3-5 of a new concept, such as a specific person, pet, or landscape, shows that a pre-trained model can learn it without sacrificing the current diversity of the model and edit the new concept or inserted it into another context (e.g. my pet surfing in Hawaii). Same goal but a different approach, \cite{gal2022image} proposed textual inversion to learn a textual embedding of the new concept, and \cite{zhang2023adding} go further with ControlNet, which not only allows you to learn a new concept with a pre-trained model but gives you additional input controls for the generation process such as canny edges. Research such as \cite{shi2023instantbooth} use adapter layers in the model architecture to reinforce the signal of the new concept without compromising the model's capabilities, in which a block is appended into the model architecture with the necessary set of weights to re-interpreted the model inference toward a signal, or more generally, a new task. However, only some downstream tasks can be easily expressed using a text prompt or a loss function. Other tasks are heavily context-dependent, challenging to define, or directly subjective. Recent work by \cite{black2023training} brings Reinforcement Learning techniques for adapting pre-trained diffusion models with challenging objectives to control via prompting, such as image compressibility or prompt alignment, and achieve promising results. Borrowing ideas from previous work and the recent development in Reinforcement Learning Human Feedback (RLHF) used by visual language models \citep{lee2023aligning}, this thesis work aims to continue this research line to fine-tuning diffusion models via rewards functions. \\

\section{Contributions and Outline}

\ca{Volver a revisar cuando termine el último capítulo y ver si se acomodan
las contribucionoes a todo lo que finalmente se hizo...}

In this thesis we will explore the use of Reinforcement Learning techniques to adapt pre-trained diffusion models to new tasks. Specifically, the use of policy optimization over agents built-on-top of the denoising network---within
the diffusion model framework---and guide the sampling process to maximize a reward signal. The main contributions of this work are:

\begin{enumerate}
    \item The neccessary background to understand the intersection between diffusion models and reinforcement learning that is an essential guide to contribute to the field.
    \item A reproduction of the work \textit{Training diffusion models with reinforcement learning} \cite{black2023training} that introduces policy optimization algorithms to adapt pre-trained diffusion models to new tasks.
    \item Empirical analysis of reward signals during the sample trajectories of the diffusion process.
    \item Incorporating baseline and the intermediate steps reward to training the policy, extending and comparing the results to the original work \citep{black2023training}.
\end{enumerate}

We first need to understand the basics of diffusion models, \textbf{Chapter 2} provide a background of this kind of generative models, how can formulate the model as a Markov chain, the training objective, how can we condition information or guide the model sample generation, and discuss improvements over the vanilla formulation as well as some of recent advances in the field. 

The next step is a quick journey through the Reinforcement Learning (RL) field in \textbf{Chapter 3}, we focus on a broad overview of the Markov Decision Processes (MDP), and then going deep in understanding Policy Optimization algorithms to solve the MDP. The goal is to understand how to train an agent capable of taking actions in response to its environment. 

We want to build an agent on top of a pretrained diffusion model that can teach the model to generate samples that satisfy the objective of our agent driven by a reward. In \textbf{Chapter 4}, equipped with the necessary background, we will explore the intersection between diffusion models and reinforcement learning. At the end, we will reproduce the work in \cite{black2023training} that introduces such conecction between the two fields using a smaller model, and then we will discuss the challenges and opportunities of this approach. 

Finally, in \textbf{Chapter 5} we will present the conclusion of this work and some future directions.
