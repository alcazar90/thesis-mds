\chapter{Introduction}

\section{Overview}

\begin{chapquote}{Morpheus, \textit{The Matrix}}
``You take the red pill, you stay in Wonderland, and I show you how deep the rabbit hole goes. Remember, all I'm offering is the truth. Nothing more.''
\end{chapquote}

\insertimage[\label{fig:anatomy-rl-algo}]{ch3-rl/anatomy-rl-algorithms.png}{scale=0.85}{Adapted from the Sergey Levine Course on Deep Reinforcement Learning}

\ca{Probablemente acá sera interesante poner ese diagrama de Venn con la interessción de modelos de difusión y RL...(?)}

\section{Related Work}

\ca{En esta sección incorporar el related work de reinforcement learning, reinforcement learning from human feedback, esta se puede completar antes 
que la sección de arriba. \textbf{TODO:} basarse en exposición y blogpost de \href{https://yang-song.net/blog/2021/score/}{Generative Modeling by Estimating Gradients of the Data Distribution} (Yang Song).}

Diffusion \citep{sohldickstein2015deep} \citep{ho2020denoising} and score-based models \citep{song2020generative} are a kind of generative models, in which they learn from the dataset $\mathcal{D}$ a distribution $p(\mathbf{x})$ and achieve the two basic primitive operations expected for this type of models, evaluation and sampling, with the importance that scale well and handle highly complex data such as images or audio. To accomplish this task, diffusion models start with a simple prior distribution (e.g. isotropic-gaussian) and, iteratively, through a denoising process, it mutates into the target distribution. Recent years have shown an essential advance in extending and adapting the work on diffusion models, likewise making the sampling process faster in the work of \cite{song2020denoising}, \cite{nichol2021improved}, and \cite{Salimans2022ProgressiveDF}. Incorporate additional information $\mathbf{y}$ (e.g. text prompt, an image seed) to learn a conditional model $p(\mathbf{x} | \mathbf{y})$; this endeavour will be explored indirectly by guiding the sampling with an external classifier in \cite{Dhariwal2021DiffusionMB}, or achieving the same but directly in the model training phase using classifier-free guidance purpose in \cite{Ho2022ClassifierFreeDG} and effectively implemented in work \cite{nichol2022glide}. These techniques had many connections with the concept of temperature commonly used in Large Language Models (LLM) and proved helpful to guide the sampling in the gradient direction toward the space between $\mathbf{x}$ and $\mathbf{y}$ signals. Moreover, progress toward improving the likelihood estimation and exhibited impressive results on tasks such as text conditional image generation with visual language models \cite{ramesh2022hierarchical} and \cite{saharia2022photorealistic}, super-resolution \cite{ho2021cascaded}, in-painting, style transfer, and combining different data modalities. \\

A vital research line is to use pre-trained diffusion models and adapt to novel tasks or concepts. Given the high cost of data recollection, the quantity and quality of these, and the computational resources it takes to train these "lossless" generative models from scratch. The idea is to push forward in the line that follows the LLM and their adaptation to downstream tasks \citep{Radford2019LanguageMA}. Specifically, the work of \cite{ruiz2023dreambooth} using a small number of images 3-5 of a new concept, such as a specific person, pet, or landscape, shows that a pre-trained model can learn it without sacrificing the current diversity of the model and edit the new concept or inserted it into another context (e.g. my pet surfing in Hawaii). Same goal but a different approach, \cite{gal2022image} proposed textual inversion to learn a textual embedding of the new concept, and \cite{zhang2023adding} go further with ControlNet, which not only allows you to learn a new concept with a pre-trained model but gives you additional input controls for the generation process such as canny edges. Research such as \cite{shi2023instantbooth} use adapter layers in the model architecture to reinforce the signal of the new concept without compromising the model's capabilities, in which a block is appended into the model architecture with the necessary set of weights to re-interpreted the model inference toward a signal, or more generally, a new task. However, only some downstream tasks can be easily expressed using a text prompt or a loss function. Other tasks are heavily context-dependent, challenging to define, or directly subjective. Recent work by \cite{black2023training} brings Reinforcement Learning techniques for adapting pre-trained diffusion models with challenging objectives to control via prompting, such as image compressibility or prompt alignment, and achieve promising results. Borrowing ideas from previous work and the recent development in Reinforcement Learning Human Feedback (RLHF) used by visual language models \citep{lee2023aligning}, this thesis work aims to continue this research line to fine-tuning diffusion models via rewards functions. \\

\section{Contributions and Outline}

\ca{Volver a revisar cuando termine el último capítulo y ver si se acomodan
las contribucionoes a todo lo que finalmente se hizo...}

This thesis explores using Reinforcement Learning (RL) techniques to adapt pre-trained diffusion models to new tasks. Specifically, it involves policy optimization over agents built on top of the denoising network within the diffusion model framework to guide the sampling process and maximize a reward signal. The main contributions are: \\

\begin{enumerate}
    \item Providing the necessary background to understand the intersection of diffusion models and reinforcement learning, serving as an essential guide for contributing to the field.
    \item Reproducing the work \textit{``Training Diffusion Models with Reinforcement Learning''} (Black, 2023 \cite{black2023training}), which introduces policy optimization algorithms (DDPO) to adapt pre-trained diffusion models to new tasks.
    \item Conducting empirical analysis of reward signals during the sample trajectories of the diffusion process.
    \item Incorporating baseline and intermediate steps rewards in training the policy, extending and comparing results to DDPO.
    \item \ca{Agregar algo de RLHF y alignment...}
\end{enumerate}

\noindent To lay the groundwork, \textbf{Chapter 2} covers the basics of diffusion models, including their formulation as a Markov chain, the training objective, conditioning information or guiding model sample generation, and improvements such as speeding up sample generation. \\

\noindent Next, a broad overview of the Reinforcement Learning field is provided in \textbf{Chapter 3}, focusing on Markov Decision Processes (MDP) and Policy Optimization algorithms. The goal is to understand how to train an agent to take actions in response to its environment. \\

\noindent Equipped with the necessary background, the intersection of diffusion models and reinforcement learning is explored in \textbf{Chapter 4}. This includes reproducing the Denoising Diffusion Policy Optimization (DDPO) \cite{black2023training} using a smaller unconditional generative model and discussing the challenges and opportunities of this approach \ca{agregar improvements si alcanzo...}. \\

\noindent Finally, \textbf{Chapter 5} presents the conclusions of this work and future directions. \\
