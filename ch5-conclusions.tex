\chapter{Conclusion}

% El principal objetivo de este trabajo fue construir todo los conocimientos
% necesarios para entender como usar reinforcement learning para controlar
% la generación de muestras de un modelo de difusión, via la maximización de
% una función de recompensa. Se construyo el background para entender qué son y cómo funcionan los modelos de difusion revisando la presentación de uno de sus trabajos fundacionales que es DDPM \cite{ho2020denoising}. Después se realizó un experimento usando una de las alternativas a RL para controlar la generación de muestras de un modelo preentrenado la cual consiste en guiarlo con un clasificador externo basado en los CLIP embeddings (ver Sección~\ref{sec:clip-cg}). Se vio brevemente como mejorar la eficiencia 
% del muestreo en los modelos de difusión usando un modelo implicito (DDIM), que una vez fijado el ruido inicial de la imagen a generar, la trayectoria de generación es determinista. Dos utilidades de esto último, (i) es posible estimar la versión denoiseada de cualquier estado intermedio de la generación de una imagen en la cadena de difusión (ver Figura~\ref{fig:sample-trajectories}), y  (ii) aprovechar la propiedad de \textit{sample consistency}, almost inversibilidad, que nos presenta DDIM para editar una image de manera controlada. Estimando el ruido inicial para una imagen objetivo y luego durante la cadena de difusión realizar las ediciones respectivas. En la Figura~\ref{fig:ddim-inversion-pascal} se experimentó para jugar con esta idea. \\

% \noindent En el dominio de reinforcement learning seguimos el camino para resolver  el MDP a través de optimizar directamente la policy. Se presentó la herramienta matemática necesaria para lograrlo, la estimación del gradiente a través de una función de score. Su formulación clásica en el dominio de RL es el algorittmo REINFORCE, en donde el objetivo es maximizar el retorno esperado del proceso generador de trayectorias. Usando una función de reward escalar arbitraria, es posible estimar el gradiente de la distribución policy, y usarlo para actualizar sus parámetros. De esta manera se van reforzando los comportamientos deseados para lograr el objetivo encodeado en el reward. Se finaliza este camino con el algoritmo PPO, el que es más eficiente en datos que REINFORCE, permitiendo realizar actualizaciones de la policy en batch y es un algoritmo de optimización de primer orden. Esta es la pieza clave que necesitamos para controlar la generación de muestras de un modelo de difusión. \\

% \noindent Vimos en el capítulo de los resultados la conexión entre modelos de difusión y RL. Formulando el proceso de generación de muestras como un sequential decision making process, donde cada acción es la elección
% de un estado intermedio de la cadena de difusión (ver Figura~\ref{fig:diffusion-model-mdp}). Esto nos permite formular 
% el MDP y dada cualquier función de recompensa que queramos maximizar, se puede
% usar la bateria de herramientas de RL para encontrar la policy que maximiza
% la recompensa esperada. La implementación de policy gradient en el contexto de los modelos de difusión se denota como denoising diffusion policy otimization (DDPO), introducida en el trabajo de referencia ``Training Diffusioon Models with Reinforcement Learning'' \cite{black2023training}. La experimentación fue usando modelo de difusión preentrenados con la metodologia DDPM como
% \href{https://huggingface.co/google/ddpm-celebahq-256}{\textbf{\texttt{google/ddpm-celebaahq-256}}} y \href{https://huggingface.co/google/ddpm-church-256}{\textbf{\texttt{google/ddpm-church-256}}}. En donde se implementó DDPO y optimizó en base a las funciones de recompensa utilizadas en el trabajo de referencia como JPEG compressibility, incompressibility, y usando LAION Aesthetic Score obteniendo buenos resultados (ver Tabla~\ref{tab:reward-results}). Esta última función de recompensa es un ejemplo del potencial de introducir al humano en el loop de generación para controlar modelo generativos a través de RL usando feedback humano (RLHF, ver Sección~\ref{sec:rlhf}). Se diseño tambien una función de recompensa adicional en base a
% un clasificador de edades de rostros humanos, un experimento cuyo objetivo fue cambiar las proporciones de rostros de famosos para generar en su mayoría rostros de personas sobre los $50$ años, la función de recompensa utilizada fue \texttt{OVER50}. El objetivo de este experimento fue demostrar la flexibilidad de RL para controlar la generación de muestras de un modelo de difusión, y la capacidad de diseñar funciones de recompensa que capturen la diversidad de objetivos que se pueden tener en mente. \\

% \noindent Se realizó un estudio empirico sobre la señal de reward durante
% el proceso de generación, en donde se observó que la señal de reward es
% muy ruidosa para los estados intermedios. Sin embaergo, al usar la señal de reward sobre las versiones denoseadas, es posible observar que la señal de reward es más estable y menos ruidosa. Esto nos da una pista de cómo mejorar el algoritmo DDPO, explorando la posibilidad de usar la señal de reward sobre los estados intermedios denoseados (Sección~\ref{sec:empirical-analysis}). Sin embargo, aún existen limitancias al utilizar este tipo de metodologias, el
% problema de la sobreoptimización y pérdida de diversidad en las muestras es
% un desafio abierto (ver Figura~\ref{fig:clip-emb-ddpo-vs-ddpm}), así como también robustecer los benchmark de evaluación para facilitar la comparación con otros métodos. Estos problemas son oportunidades para continuar explorando los desafíos de un campo cada vez más activo y que intersecta modelos de difusión, reinforcement learning, e interacción humano computador (HCI).  \\ 


The primary goal of this work was to build the foundational knowledge necessary to understand how to use reinforcement learning (RL) to control the generation of samples from a diffusion model by maximizing a reward function. We began by establishing the background to understand what diffusion models are and how they work, with a review of one of the seminal papers in this field, DDPM \cite{ho2020denoising}. Following this, we conducted an experiment using an alternative to RL for controlling sample generation in a pretrained model, which involves guiding it with an external classifier based on CLIP embeddings (see Section~\ref{sec:clip-cg}). \\

\noindent We briefly explored how to improve the efficiency of sampling in diffusion models by using an implicit model (DDIM \cite{song2020denoising}). With DDIM, once the initial noise for the image to be generated is fixed, the generation trajectory becomes deterministic. This approach has two key benefits: (i) it allows us to estimate the denoised version of any intermediate state in the image generation chain (see Figure~\ref{fig:sample-trajectories}), and (ii) it leverages the property of sample consistency, which is almost reversible, enabling controlled image editing. This idea was tested in Figure~\ref{fig:ddim-inversion-pascal}. \\

\noindent In the realm of reinforcement learning, we followed the path to solving the Markov Decision Process (MDP) by directly optimizing the policy. We introduced the necessary mathematical tools to achieve this, specifically gradient estimation via score function. The classical formulation of this in RL is the REINFORCE algorithm, where the goal is to maximize the expected return of the trajectory-generating process. By using an arbitrary scalar reward function, we can estimate the policy gradient and use it to update the parameters, thereby reinforcing desired behaviors to achieve the objective encoded in the reward. This journey culminated with the PPO algorithm, which is more data-efficient than REINFORCE, allows for batch policy updates, and is a first-order optimization algorithm. This is the crucial piece we needed to control sample generation in a diffusion model. \\

\noindent In the results chapter, we explored the connection between diffusion models and RL, framing the sample generation process as a sequential decision-making process where each action corresponds to selecting an intermediate state in the diffusion chain (see Figure~\ref{fig:diffusion-model-mdp}). This perspective enables us to formulate the MDP and, given any reward function we wish to maximize, apply RL tools to find the policy that maximizes the expected reward. The implementation of policy gradient in the context of diffusion models is referred to as Denoising Diffusion Policy Optimization (DDPO), introduced in the reference work “Training Diffusion Models with Reinforcement Learning” \cite{black2023training}. We conducted experiments using pretrained diffusion models based on the DDPM methodology, such as \href{https://huggingface.co/google/ddpm-celebahq-256}{\textbf{\texttt{google/ddpm-celebaahq-256}}} and \href{https://huggingface.co/google/ddpm-church-256}{\textbf{\texttt{google/ddpm-church-256}}}. DDPO was implemented and optimized based on reward functions used in the reference work, such as JPEG compressibility, incompressibility, and the LAION Aesthetic Score, yielding promising results (see Table~\ref{tab:reward-results}). The latter reward function exemplifies the potential of incorporating human input into the generation loop to control generative models through RL using human feedback (RLHF, see Section~\ref{sec:rlhf}). \\

\noindent Additionally, we designed a new reward function based on a classifier of human face ages \cite{vitage-classifier-hf}, conducting an experiment aimed at altering the proportion of famous faces generated to predominantly produce images of individuals over 50 years old, with the reward function used named \texttt{OVER50}. The goal of this experiment was to demonstrate the flexibility of RL in controlling sample generation in a diffusion model and the ability to design reward functions that capture diverse objectives. \\

\noindent We also conducted an empirical study of the reward signal during the generation process, observing that the reward signal is highly noisy for intermediate states. However, when applying the reward signal to denoised versions of these states, the signal became more stable and less noisy. This observation provides a clue for improving the DDPO algorithm by exploring the possibility of using the reward signal on denoised intermediate states (Section~\ref{sec:empirical-analysis}). \\

\noindent Despite these advancements, there are still challenges with using these methodologies. The problem of over-optimization and the loss of diversity in samples remains an open challenge (see Figure~\ref{fig:clip-emb-ddpo-vs-ddpm}), as does the need to strengthen evaluation benchmarks to facilitate comparison with other methods. These challenges present opportunities for further exploration in a rapidly evolving field that intersects diffusion models, reinforcement learning, and human-computer interaction (HCI). \\